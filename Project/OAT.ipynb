{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "\n",
    "from models.cifar10.resnet_OAT import ResNet34OAT\n",
    "from models.svhn.wide_resnet_OAT import WRN16_8OAT\n",
    "from models.stl10.wide_resnet_OAT import WRN40_2OAT\n",
    "\n",
    "from dataloaders.cifar10 import cifar10_dataloaders\n",
    "from dataloaders.svhn import svhn_dataloaders\n",
    "from dataloaders.stl10 import stl10_dataloaders\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.context import ctx_noparamgrad_and_eval\n",
    "from utils.sample_lambda import element_wise_sample_lambda, batch_wise_sample_lambda\n",
    "from attacks.pgd import PGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '1'\n",
    "cpus = 8\n",
    "dataset = 'cifar10' #choices=['cifar10', 'svhn', 'stl10']\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "decay_epochs = [50, 150]\n",
    "opt = 'sgd' #choices=['sgd', 'adam']\n",
    "decay = 'cos' #choices=['cos', 'multisteps']\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "wd = 5e-4 #weight decay\n",
    "targeted = True #if true, targeted attack\n",
    "eps = 31\n",
    "steps =7\n",
    "distribution = 'disc'\n",
    "lambda_choices = [0.0,0.1,0.2,0.3,0.4,1.0]\n",
    "probs = -1\n",
    "encoding = 'rand' #choices=['none', 'onehot', 'dct', 'rand']\n",
    "dim = 128\n",
    "use2BN = True\n",
    "sampling ='ew' #sampling scheme for Lambda, choices=['ew', 'bw']\n",
    "resume = False #If true, resume from early stopped ckpt\n",
    "efficient = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'cifar10':\n",
    "    train_loader, val_loader, _ = cifar10_dataloaders(train_batch_size=batch_size, num_workers=cpus)\n",
    "elif dataset == 'svhn':\n",
    "    train_loader, val_loader, _ = svhn_dataloaders(train_batch_size=batch_size, num_workers=cpus)\n",
    "elif dataset == 'stl10':\n",
    "    train_loader, val_loader = stl10_dataloaders(train_batch_size=batch_size, num_workers=cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoding in ['onehot', 'dct', 'rand']:\n",
    "    FiLM_in_channels = dim\n",
    "else: # non encoding\n",
    "    FiLM_in_channels = 1\n",
    "if dataset == 'cifar10':\n",
    "    model_fn = ResNet34OAT\n",
    "elif dataset == 'svhn':\n",
    "    model_fn = WRN16_8OAT\n",
    "elif dataset == 'stl10':\n",
    "    model_fn = WRN40_2OAT\n",
    "model = model_fn(use2BN=use2BN, FiLM_in_channels=FiLM_in_channels).cuda()\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "model_str = os.path.join(model_fn.__name__)\n",
    "if use2BN:\n",
    "    model_str += '-2BN'\n",
    "if opt == 'sgd':\n",
    "    opt_str = 'e%d-b%d_sgd-lr%s-m%s-wd%s' % (epochs, batch_size, lr, momentum, wd)\n",
    "elif opt == 'adam':\n",
    "    opt_str = 'e%d-b%d_adam-lr%s-wd%s' % (epochs, batch_size, lr, wd)\n",
    "if decay == 'cos':\n",
    "    decay_str = 'cos'\n",
    "elif decay == 'multisteps':\n",
    "    decay_str = 'multisteps-%s' % decay_epochs\n",
    "attack_str = 'targeted' if targeted else 'untargeted' + '-pgd-%s-%d' % (eps, steps)\n",
    "lambda_str = '%s-%s-%s' % (distribution, sampling, lambda_choices)\n",
    "if probs > 0:\n",
    "    lambda_str += '-%s' % probs\n",
    "if encoding in ['onehot', 'dct', 'rand']:\n",
    "    lambda_str += '-%s-d%s' % (encoding, dim)\n",
    "save_folder = os.path.join(os.getcwd(), 'OAT_results', dataset, model_str, '%s_%s_%s_%s' % (attack_str, opt_str, decay_str, lambda_str))\n",
    "print(save_folder)\n",
    "create_dir(save_folder)\n",
    "\n",
    "# encoding matrix:\n",
    "if encoding == 'onehot':\n",
    "    I_mat = np.eye(dim)\n",
    "    encoding_mat = I_mat\n",
    "elif encoding == 'dct':\n",
    "    from scipy.fftpack import dct\n",
    "    dct_mat = dct(np.eye(dim), axis=0)\n",
    "    encoding_mat = dct_mat\n",
    "elif encoding == 'rand':\n",
    "    rand_mat = np.random.randn(dim, dim)\n",
    "    np.save(os.path.join(save_folder, 'rand_mat.npy'), rand_mat)\n",
    "    rand_otho_mat, _ = np.linalg.qr(rand_mat)\n",
    "    np.save(os.path.join(save_folder, 'rand_otho_mat.npy'), rand_otho_mat)\n",
    "    encoding_mat = rand_otho_mat\n",
    "elif encoding == 'none':\n",
    "    encoding_mat = None\n",
    "\n",
    "if distribution == 'disc':\n",
    "    val_lambdas = lambda_choices\n",
    "else:\n",
    "    val_lambdas = [0,0.2,0.5,1]\n",
    "\n",
    "if opt == 'sgd':\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
    "elif opt == 'adam':\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "if decay == 'cos':\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "elif decay == 'multisteps':\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, decay_epochs, gamma=0.1)\n",
    "\n",
    "if resume:\n",
    "    last_epoch, best_TA, best_ATA, training_loss, val_TA, val_ATA \\\n",
    "         = load_ckpt(model, optimizer, scheduler, os.path.join(save_folder, 'latest.pth'))\n",
    "    start_epoch = last_epoch + 1\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    training_loss, val_TA, val_ATA, best_TA, best_ATA = [], {}, {}, {}, {}\n",
    "    for val_lambda in val_lambdas:\n",
    "        val_TA[val_lambda], val_ATA[val_lambda], best_TA[val_lambda], best_ATA[val_lambda] = [], [], 0, 0\n",
    "\n",
    "\n",
    "attacker = PGD(eps=eps/1000, steps=steps, use_FiLM=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    train_fp = open(os.path.join(save_folder, 'train_log.txt'), 'a+')\n",
    "    val_fp = open(os.path.join(save_folder, 'val_log.txt'), 'a+')\n",
    "    start_time = time.time()\n",
    "    ## training:\n",
    "    model.train()\n",
    "    requires_grad_(model, True)\n",
    "    accs, accs_adv, losses, lps = AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "        # sample _lambda:\n",
    "        if sampling == 'ew':\n",
    "            _lambda_flat, _lambda, num_zeros = element_wise_sample_lambda(distribution, lambda_choices, encoding_mat, \n",
    "                batch_size=batch_size, probs=probs)\n",
    "        if use2BN:\n",
    "            idx2BN = num_zeros\n",
    "        else:\n",
    "            idx2BN = None\n",
    "\n",
    "        # logits for clean imgs:\n",
    "        logits = model(imgs, _lambda, idx2BN)\n",
    "        # clean loss:\n",
    "        lc = F.cross_entropy(logits, labels, reduction='none')\n",
    "        \n",
    "        if efficient:\n",
    "            # generate adversarial images:\n",
    "            with ctx_noparamgrad_and_eval(model):\n",
    "                if use2BN:\n",
    "                    imgs_adv = attacker.attack(model, imgs[num_zeros:], labels=labels[num_zeros:], _lambda=_lambda[num_zeros:], idx2BN=0)\n",
    "                else:\n",
    "                    imgs_adv = attacker.attack(model, imgs[num_zeros:], labels=labels[num_zeros:], _lambda=_lambda[num_zeros:], idx2BN=None)\n",
    "            # logits for adv imgs:\n",
    "            logits_adv = model(imgs_adv.detach(), _lambda[num_zeros:], idx2BN=0)\n",
    "            \n",
    "            # loss and update:\n",
    "            la = F.cross_entropy(logits_adv, labels[num_zeros:], reduction='none') \n",
    "            la = torch.cat([torch.zeros((num_zeros,)).cuda(), la], dim=0)\n",
    "        else:\n",
    "            # generate adversarial images:\n",
    "            with ctx_noparamgrad_and_eval(model):\n",
    "                imgs_adv = attacker.attack(model, imgs, labels=labels, _lambda=_lambda, idx2BN=idx2BN)\n",
    "            # logits for adv imgs:\n",
    "            logits_adv = model(imgs_adv.detach(), _lambda, idx2BN=idx2BN)\n",
    "\n",
    "            # loss and update:\n",
    "            la = F.cross_entropy(logits_adv, labels, reduction='none') \n",
    "        wc = (1-_lambda_flat)\n",
    "        wa = _lambda_flat\n",
    "        loss = torch.mean(wc * lc + wa * la) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # get current lr:\n",
    "        current_lr = scheduler.get_lr()[0]\n",
    "\n",
    "        # metrics:\n",
    "        accs.append((logits.argmax(1) == labels).float().mean().item())\n",
    "        if efficient:\n",
    "            accs_adv.append((logits_adv.argmax(1) == labels[num_zeros:]).float().mean().item())\n",
    "        else:\n",
    "            accs_adv.append((logits_adv.argmax(1) == labels).float().mean().item())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            train_str = 'Epoch %d-%d | Train | Loss: %.4f, SA: %.4f, RA: %.4f' % (\n",
    "                epoch, i, losses.avg, accs.avg, accs_adv.avg)\n",
    "            print(train_str)\n",
    "            # print('idx2BN:', idx2BN)\n",
    "            train_fp.write(train_str + '\\n')\n",
    "        # if i % 100 == 0:\n",
    "        #     print('_lambda_flat:', _lambda_flat.size(), _lambda_flat[0:10].data.data.cpu().numpy().squeeze())\n",
    "        #     print('_lambda:', _lambda.size(), _lambda[0:5,:].data.cpu().numpy().squeeze())\n",
    "\n",
    "    # lr schedualr update at the end of each epoch:\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    ## validation:\n",
    "    model.eval()\n",
    "    requires_grad_(model, False)\n",
    "    print(model.training)\n",
    "\n",
    "    eval_this_epoch = (epoch % 10 == 0) or (epoch>=int(0.75*epochs)) # boolean\n",
    "    \n",
    "    if eval_this_epoch:\n",
    "        val_accs, val_accs_adv = {}, {}\n",
    "        for val_lambda in val_lambdas:\n",
    "            val_accs[val_lambda], val_accs_adv[val_lambda] = AverageMeter(), AverageMeter()\n",
    "            \n",
    "        for i, (imgs, labels) in enumerate(val_loader):\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "            for j, val_lambda in enumerate(val_lambdas):\n",
    "                # sample _lambda:\n",
    "                if distribution == 'disc' and encoding_mat is not None:\n",
    "                    _lambda = np.expand_dims( np.repeat(j, labels.size()[0]), axis=1 ).astype(np.uint8)\n",
    "                    _lambda = encoding_mat[_lambda,:] \n",
    "                else:\n",
    "                    _lambda = np.expand_dims( np.repeat(val_lambda, labels.size()[0]), axis=1 )\n",
    "                _lambda = torch.from_numpy(_lambda).float().cuda()\n",
    "                if use2BN:\n",
    "                    idx2BN = int(labels.size()[0]) if val_lambda==0 else 0\n",
    "                else:\n",
    "                    idx2BN = None\n",
    "                # TA:\n",
    "                logits = model(imgs, _lambda, idx2BN)\n",
    "                val_accs[val_lambda].append((logits.argmax(1) == labels).float().mean().item())\n",
    "                # ATA:\n",
    "                # generate adversarial images:\n",
    "                with ctx_noparamgrad_and_eval(model):\n",
    "                    imgs_adv = attacker.attack(model, imgs, labels=labels, _lambda=_lambda, idx2BN=idx2BN)\n",
    "                linf_norms = (imgs_adv - imgs).view(imgs.size()[0], -1).norm(p=np.Inf, dim=1)\n",
    "                logits_adv = model(imgs_adv.detach(), _lambda, idx2BN)\n",
    "                val_accs_adv[val_lambda].append((logits_adv.argmax(1) == labels).float().mean().item())\n",
    "\n",
    "    val_str = 'Epoch %d | Validation | Time: %.4f | lr: %s' % (epoch, (time.time()-start_time), current_lr)\n",
    "    if eval_this_epoch:\n",
    "        val_str += ' | linf: %.4f - %.4f\\n' % (torch.min(linf_norms).data, torch.max(linf_norms).data)\n",
    "        for val_lambda in val_lambdas:\n",
    "            val_str += 'val_lambda%s: SA: %.4f, RA: %.4f\\n' % (val_lambda, val_accs[val_lambda].avg, val_accs_adv[val_lambda].avg)\n",
    "    print(val_str)\n",
    "    val_fp.write(val_str + '\\n')\n",
    "    val_fp.close() # close file pointer\n",
    "\n",
    "    # save loss curve:\n",
    "    training_loss.append(losses.avg)\n",
    "    plt.plot(training_loss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_folder, 'training_loss.png'))\n",
    "    plt.close()\n",
    "\n",
    "    if eval_this_epoch:\n",
    "        for val_lambda in val_lambdas:\n",
    "            val_TA[val_lambda].append(val_accs[val_lambda].avg) \n",
    "            plt.plot(val_TA[val_lambda], 'r')\n",
    "            val_ATA[val_lambda].append(val_accs_adv[val_lambda].avg)\n",
    "            plt.plot(val_ATA[val_lambda], 'g')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Validation Accuracy')\n",
    "            plt.legend([\"SA\", \"RA\"])\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_folder, 'val_acc%s.png' % val_lambda))\n",
    "            plt.close()\n",
    "    else:\n",
    "        for val_lambda in val_lambdas:\n",
    "            val_TA[val_lambda].append(val_TA[val_lambda][-1]) \n",
    "            plt.plot(val_TA[val_lambda], 'r')\n",
    "            val_ATA[val_lambda].append(val_ATA[val_lambda][-1])\n",
    "            plt.plot(val_ATA[val_lambda], 'g')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Validation Accuracy')\n",
    "            plt.legend([\"SA\", \"RA\"])\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_folder, 'val_acc%s.png' % val_lambda))\n",
    "            plt.close()\n",
    "\n",
    "    # save pth:\n",
    "    if eval_this_epoch:\n",
    "        for val_lambda in val_lambdas:\n",
    "            if val_accs[val_lambda].avg >= best_TA[val_lambda]:\n",
    "                best_TA[val_lambda] = val_accs[val_lambda].avg # update best TA\n",
    "                torch.save(model.state_dict(), os.path.join(save_folder, 'best_SA%s.pth' % val_lambda))\n",
    "            if val_accs_adv[val_lambda].avg >= best_ATA[val_lambda]:\n",
    "                best_ATA[val_lambda] = val_accs_adv[val_lambda].avg # update best ATA\n",
    "                torch.save(model.state_dict(), os.path.join(save_folder, 'best_RA%s.pth' % val_lambda))\n",
    "    save_ckpt(epoch, model, optimizer, scheduler, best_TA, best_ATA, training_loss, val_TA, val_ATA, \n",
    "        os.path.join(save_folder, 'latest.pth'))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
